{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考博客https://www.cnblogs.com/pinard/p/6140514.html，\n",
    "https://blog.csdn.net/qq_22238533/article/details/79185969"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成学习（ensemble learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成学习是通过构建并结合多个学习器来完成学习任务.一般结构：先产生一组“个体学习器”，在以某种策略将他们结合起来.**同质**个体学习器，也称为基学习器(base learner),相应的算法称为基学习算法.当然也可以是异质的，此时个体学器称为“组件学习器”.   \n",
    "基学习器有时也被称为**弱学习器**，集成学习通过多个学习器的结合，常常可以获得比单一学习器显著**优越的泛化性能**,这对弱学习器尤为明显."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 误差率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑二分类问题$y\\in\\{-1,+1\\}$，假定基分类器的分类误差率为$\\epsilon$,即对每一个基分类器$G_i$有：$$P(G_i(x)\\neq y)=\\epsilon$$\n",
    "假定集成通过简单投票法来结合$T$个基分类器，若有超过半数的基分类器正确，则集成分类就正确，这里使用符号函数$sign$来表示投票过程：$$G(x)=sign(\\sum_{i=1}^TG_i(x))$$\n",
    "集成学习预测错误的条件是：k个基分类器预测正确，其中$k\\leq \\left \\lfloor \\frac{T}{2} \\right \\rfloor$,$T-k$个基分类器预测错误."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设基分类器的错误率相互独立，则集成学习器分类误差率为：$$p(H(x)\\neq y)=\\sum_{k=0}^{\\left \\lfloor \\frac{T}{2} \\right \\rfloor}C_T^k(1-\\epsilon)^k\\epsilon^{(T-k)}  （1）$$这里对上式的理解是，每个基分类器误分概率是$\\epsilon$，所以正确分类的概率为$(1-\\epsilon)$,那么要想求整体误分概率，由于是投票方式集成，因此只需要误分的基分类器数量大于正确分类的基分类器数量就可以，所以是个组合问题."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据Hoeffding不等式有：$$p(H(x)\\neq y)=\\sum_{k=0}^{\\left \\lfloor \\frac{T}{2} \\right \\rfloor}C_T^k(1-\\epsilon)^k\\epsilon^{(T-k)}\\leq exp(-\\frac{1}{2}T(1-2\\epsilon)^2)  （2）$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解释：对于服从伯努利分布（1或0）抛硬币游戏，如果过设置单次抛掷硬币A面向上的概率是$p$，则B面向上为$1-p$。那么抛掷$n$次，A面朝上的次数不超过$k$次的概率为：$$P(H(n)\\leq k)=\\sum_{i=0}^kC_n^ip^i(1-p)^{n-i}$$，这里$H(n)$表示n次抛硬币A面向上次数.\n",
    "对于$k$，存在某一$\\gamma$，当$k=(p-\\gamma)n$时，有:$$P(H(n)\\leq k)=P(H(n)\\leq (p-\\gamma)n)\\leq exp(-2\\gamma^2n)$$这就是伯努利分布的Hoeffding不等式。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应（1）式中，把T个分类器的判断看为T次抛硬币，A面向上的次数小于等于$\\frac{T}{2}$,所以存在有$$\\frac{T}{2}=(\\epsilon-\\gamma)T$$,解出：$$\\gamma=\\epsilon-\\frac{1}{2}$$\n",
    "所以可以改写为$$P(H(T)\\leq \\frac{T}{2})\\leq exp(-2(\\epsilon-\\frac{1}{2})^2T)$$，所以有（2）式成立.**(2)式显示出，随着集成中个体分类器数目$T$的增大，集成的误差率将指数级下降(因为误差率的上界是指数函数)，最终趋于0，但是这建立在关键性假设上：基学习器的误差相互独立.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**集成学习大体可以分为两类：   \n",
    "1.个体学习器之间不存在强依赖关系，可以同时生成的并行化方法,代表是Bagging和随机森林（RandomForests）（并行生成）   \n",
    "2.个体学习器之间存在强依赖关系，必须串行生成的序列化方法，每一轮迭代产生一个个体学习器,代表方法是Boosting（串行生成）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一 随机森林（Random Forests）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why   \n",
    "树模型一般来说是高方差，低偏差的，也就是容易过拟合.而且分类树模型的另一个缺点是它们是不稳定的，也是就是当数据发生微小变化时，会导致另外一个完全不同的树模型.而随机森林是可以解决不稳定性和过拟合的最有效方法之一."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林基于两个基本概念，称为bagging和subspace sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging  \n",
    "给定包含$m$个样本的数据集，我们先随机选取一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时，该样本仍有可能被选中，这样经过$m$次随机采样，我们得到含有$m$个样本的采样集.这样有的样本会多次出现.我们可以采集$T$个含有$m$个样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合.Bagging对于分类通常也是采取简单投票方式来集成基学习器，而对于回归则采取平均来集成基学习器."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林是Bagging的一个扩展变体.RF以决策树作为基学习器然后构建Bagging集成。最重要的是**在构建决策树过程中，引入随机属性选择**，也即是传统决策树选择划分属性时，是在当前节点的属性集合中选择一个最优属性(设总共有$d$个属性)，而在RF中，对于决策树的每个节点，先从该节点的属性集合中随机选择一个包含$k$个属性的子集，然后再从这个子集中选择一个最优属性用于划分.这里参数$k$控制随机性程度，当$k=d$时，与传统构建一样，当$k=1$时，则是随机选择一个属性划分.一般选取$k=log_2(d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 极致随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在极随机化的树中（参见ExtraTreesClassifier和ExtraTreesRegressor类），随机性在计算分割的方式上更进了一步。 与在随机森林中一样，使用候选特征的随机子集，但是不是寻找最具辨别力的阈值，而是针对每个候选特征随机绘制阈值，并且挑选这些随机生成的阈值中的最佳阈值作为分裂规则。这通常允许更多地减少模型的方差，代价是偏差略微增加（参考Sklearn documentation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单，易实现，计算开销小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二 提升方法（Boosting）-AdaBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提升方法就是从弱学习算法出发，反复学习，得到一系弱分类器（基分类器），然后组合这些弱分类器，构成强分类器。**大多数提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提升方法要解决的两个问题：   \n",
    "1.在每一轮如何改变训练数据的权值或概率分布   \n",
    "2.如何将弱分类器组合成强分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoosting的解决方式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值，这样可以加大后一轮的弱分类器对错误分类样本的关注   \n",
    "2.AdaBoosting采用加权多数表决的方法.具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大作用，减小分类误差率大的弱分类器权值，使其在表决中起较小作用."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (一) AdaBoosting算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost二类二分类问题算法   \n",
    "假设给定一个二分类的训练数据集:$$T=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}$$其中$y\\in\\{+1，-1\\}$     \n",
    "输入：训练数据集$T=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}$；弱学习算法；   \n",
    "输出：最终分类器$G(x)$   \n",
    "1.初始化训练数据的权值分布$$D_1={w_{11},...,w_{1i},...,w_{1n}},w_{1i}=\\frac{1}{N},i=1,2,..,N$$\n",
    "2.对$m=1,2，...，M$:   \n",
    "$\\quad\\quad$(a).使用具有权值分布$D_m$的训练数据学习，得到基分类器：$$G_m(x):X\\rightarrow{+1,-1}$$\n",
    "$\\quad\\quad$(b).计算$G_m(x)$在训练数据集上的分类误差率：$$e_m=P(G_m(x)\\neq y)=\\sum_{i=1}^Nw_{mi}I(G_m(x_i)\\neq y_i)（也就是误分类样本的权值之和）  （1）$$\n",
    "$\\quad\\quad$(c).计算$G_m(x)$的系数：$$\\alpha_m=\\frac{1}{2}log\\frac{1-e_m}{e_m}  （2）$$   \n",
    "$\\quad\\quad$(d).更新训练数据集的权值分布：$$D_1={w_{m+1,1},...,w_{m+1,i},...,w_{m+1,n}}$$ $$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_my_iG_m(x_i)),i=1,2..,N   （4）$$ $$即 ：w_{m+1,i}=\\left\\{\\begin{matrix}\n",
    "\\frac{w_{mi}}{Z_m}e^{-\\alpha_m},G_m(x_i)=y_i\\\\ \n",
    "\\frac{w_{mi}}{Z_m}e^{\\alpha_m},G_m(x_i)\\neq y_i\n",
    "\\end{matrix}\\right.$$\n",
    "$\\quad\\quad$**相对正确分类样本，误分类样本权值被放大$e^{2\\alpha_m}$倍**，这里$Z_m$是规范化因子（目的是使权值在$0-1$之间且和为$1$）$$Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_my_iG_m(x_i))  （5）$$\n",
    "3.构建基本分类器的线性组合：$$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)  （6）$$**注意线性组合实现$M$个基本分类器的加权表决，其符号表示实例$x$的类别，$f(x)$的值表示分类的确信度，$\\alpha_m$之和并不为1**   \n",
    "得到最终分类器：$$G(x)=sign(f(x))=sign(\\sum_{m=1}^M\\alpha_mG_m(x))  （7）$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于AdaBoost多元分类算法，原理与二元分类似，主要区别在弱分类器系数上.比如AdaBoost SAMME算法，其弱分类器系数：\n",
    "$$\\alpha_m=\\frac{1}{2}log(\\frac{1-e_m}{e_m})+log(K-1)$$其中$K$表示类别数，当为二分类时$K=2$,此时系数$\\alpha$就和原算法一样了.论文http://web.stanford.edu/~hastie/Papers/samme.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost回归问题算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用AdaBoost R2回归   \n",
    "输入：训练数据集$T=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}$；弱学习算法；   \n",
    "输出：最终强学习器G(x)   \n",
    "1.初始化训练数据的权值分布$$D_1={w_{11},...,w_{1i},...,w_{1n}},w_{1i}=\\frac{1}{N},i=1,2,..,N$$\n",
    "2.对$m=1,2，...，M$:   \n",
    "$\\quad\\quad$(a).使用具有权值分布$D_m$的训练数据学习，得到弱分类器$G_m(x)$   \n",
    "$\\quad\\quad$(b).计算$G_m(x)$在训练数据集上每个样本的相对误差：   \n",
    "$\\quad\\quad\\quad$线性误差$$e_{mi}=\\frac{y_i-G_m(x_i)}{E_m},i=1,2,...,N$$   \n",
    "$\\quad\\quad\\quad$平方误差$$e_{mi}=\\frac{(y_i-G_m(x_i))^2}{E_m^2},i=1,2,...,N$$   \n",
    "$\\quad\\quad\\quad$指数误差$$e_{mi}=1-exp(\\frac{-(y_i-G_m(x_i))}{E_m}),i=1,2,...,N$$ \n",
    "$\\quad\\quad\\quad$其中$E_m$是训练集上最大误差$$E_m=max|y_i-G_m(x_i)|,i=1,2,...,N$$\n",
    "$\\quad\\quad$(c).计算$G_m(x)$在训练数据集上回归误差率： $$e_m=\\sum_{i=1}^Nw_{mi}e_{mi}$$\n",
    "$\\quad\\quad$(d).计算弱学习器权重系数$$\\alpha_m=\\frac{e_m}{1-e_m}$$\n",
    "$\\quad\\quad$(e).更新样本集权重分布$$w_{m+1,i}=\\frac{w_{m,i}}{Z_m}\\alpha_{m}^{1-e_{mi}},i=1,2,...,N$$\n",
    "$$Z_m=\\sum_{i=1}^Nw_{m,i}\\alpha_{m}^{1-e_{mi}},i=1,2,...,N$$\n",
    "3.构建最终强学习器$$G(x)=\\sum_{m=1}^M(ln\\frac{1}{\\alpha_m})G_m(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import math\n",
    "e=np.arange(0.1,1,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha_m=\\frac{1}{2}log\\frac{1-e_m}{e_m}$的图像，$e_m\\leq\\frac{1}{2}$时，$\\alpha\\geq0$,而且$\\alpha$随着$e_m$的减小而增大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a=0.5*np.log((1-e)/(e))\n",
    "pl.plot(e,a)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost误差分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定理1 $\\quad \\quad$AdaBoost算法最终分类器的训练误差界为：**   \n",
    "$$\\frac{1}{N}\\sum_{i=1}^NI(G(x_i)\\neq y_i)\\leq\\frac{1}{N}\\sum_{i=1}^N exp(-y_if(x_i))=\\prod_mZ_m$$这里$G(x)$来自式(7)，$f(x)$来自式(6),$Z_m$来自(5).**这一定理说明可以在每一轮选取适当$G_m$使得$Z_m$最小，从而使训练误差下降最快**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "证明：   \n",
    "当$G(x_i)\\neq y_i$时$I=1$，$y_if(x_i)<0$,此时$exp(-y_if(x_i))\\geq1$,当$G(x_i)\\neq y_i$时$I=0$，$y_if(x_i)>0$,此时$exp(-y_if(x_i))\\leq1$.$$\\frac{1}{N}\\sum_{i=1}^NI(G(x_i)\\neq y_i)\\leq 1$$由此直接推出前一部分."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后一部分：  \n",
    "根据(4)式我们有$$w_{mi}exp(-\\alpha_my_iG_m(x_i))=Z_mw_{m+1,i}（8）$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推导：$$\\frac{1}{N}\\sum_i^Nexp(-y_if(x_i))=\\frac{1}{N}\\sum_i^Nexp(-\\sum_{m=1}^M\\alpha_my_iG_m(x_i))$$\n",
    "$$=\\frac{1}{N}\\sum_i^N\\prod_{m=1}^Mexp(-\\alpha_my_iG_m(x_i))$$由于$w_{1,i}=\\frac{1}{N}$所以有$$=\\sum_i^Nw_{1i}\\prod_{m=1}^Mexp(-\\alpha_my_iG_m(x_i))$$再是使用（8）式则有$$=Z_1\\sum_i^Nw_{2i}\\prod_{m=2}^Mexp(-\\alpha_my_iG_m(x_i))$$ $$=Z_1Z_2\\sum_i^Nw_{3i}\\prod_{m=3}^Mexp(-\\alpha_my_iG_m(x_i))$$\n",
    "$$=...$$\n",
    "$$=\\prod_{m=1}^MZ_m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定理2** $\\quad\\quad$  **对于二分类问题的AdaBoost训练误差界有**："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\prod_{m=1}^MZ_m=\\prod_{m=1}^{M}[2\\sqrt {e_m(1-e_m)}]=\\prod_{m=1}^{M}\\sqrt(1-4\\gamma_m^2)\\leq exp(-2\\sum_{m=1}^M\\gamma_m^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里$\\gamma_m=\\frac{1}{2}-e_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "证明：由(5)式有：$$Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_my_iG_m(x_i)) $$ $$=\\sum_{G(x_i)=y_i}w_{mi}e^{-\\alpha_m}+\\sum_{G(x_i)\\neq y_i}w_{mi}e^{\\alpha_m}$$ $$=(1-e_m)e^{-\\alpha_m}+e_me^{\\alpha_m}$$ $$=2\\sqrt{e_m(1-e_m)}=\\sqrt{1-4\\gamma_m^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据$e^x和\\sqrt{1-x}$在x=0处的泰勒展开式推出不等式$\\sqrt{(1-4\\gamma_m^2)}\\leq exp(-2\\gamma_m^2)$   \n",
    "**推论**$\\quad\\quad$**如果存在$\\gamma>0$对所有$\\gamma<\\gamma_m$则**：$$\\frac{1}{N}\\sum_{i=1}^NI(G(x_i)\\neq y_i)\\leq exp(-2M\\gamma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这表明此条件（二分类）下AdaBoost的训练误差是以指数速率下降的，这是训练误差上界是指数函数.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost算法的解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost算法另一个解释：AdaBoost是模型为加法模型，损失函数为指数函数，学习算法为前向分步算法的二分类学习方法.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向分步算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑加法模型:$$f(x)=\\sum_{m=1}^M\\beta_mb(x;\\gamma_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$b(x;\\gamma_m)$为基函数，$\\gamma_m$为基函数参数，$\\beta_m$为基函数系数.显然(6)式：$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x) $也是一个加法模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在给定训练数据及损失函数$L(y,f(y))$的条件下，学习加法模型$f(x)$的问题，**成为经验风险极小化即损失函数极小化问题**：$$min_{\\beta_m,\\gamma_m}\\sum_{i=1}^NL(y_i,\\sum_{m=1}^M\\beta_mb(x;\\gamma_m))  （14）$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 前向分布算法求解这个问题思路：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式(14)，那么就可以简化优化的复杂度.具体的，每一步只需要优化如下损失函数：$$min_{\\beta,\\gamma}\\sum_{i=1}^NL(y_i,\\beta b(x_i,\\gamma))  （15）$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**具体的前向分布算法:**  \n",
    "输入：训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}$；损失函数$L(y,f(x))$;基本函数集${b(x;\\gamma)}$;   \n",
    "输出：加法模型$f(x)$.   \n",
    "(1)初始化$f_0(x)=0$  \n",
    "(2)对$m=1,2,...,M$:   \n",
    "$\\quad\\quad$a.极小化损失函数$$argmin_{\\beta,\\gamma}\\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma))  （16）$$ $\\quad\\quad$得到参数$\\beta_m$,$\\gamma_m$   \n",
    "$\\quad\\quad$b.更新$$f_m(x)=f_{m-1}(x)+\\beta_mb(x;\\gamma_m)  （17）$$\n",
    "(3).得到加法模型$$f(x)=f_M(x)=\\sum_{m=1}^M\\beta_mb(x;\\gamma_m) （18）$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost与前向分步算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定理3**$\\quad\\quad$ **AdaBoost是前向分步算法的特例.AdaBoost模型是基分类器组成的加法模型，损失函数是指数函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 证明:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于AdaBoost的最终分类器是$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)$,本身就是一个加法模型，因此我们只需要证明其损失函数是指数函数$$L(y,f(x))=exp(-yf(x))$$时，其学习的具体操作等价于AdaBoost算法学习的具体操作."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设经过$m-1$轮迭代前向分布算法已经得到$f_{m-1}(x)$:$$f_{m-1}(x)=f_{m-2}(x)+\\alpha_{m-1}G_{m-1}(x)=\\alpha_1G_1(x)+...+\\alpha_{m-1}G_{m-1}(x)$$\n",
    "第m轮迭代可以得到$\\alpha_m$，$G_m(x)$和$f_m(x)$:$$f_m(x)=f_{m-1}(x)+\\alpha_mG_m(x)$$目标是使前向分步算法得到的$\\alpha_m$和$G_m(x)$使$f_m(x)$在训练数据集$T$上的指数损失函数最小，即$$(\\alpha_m,G_m(x))=argmin_{\\alpha,G}\\sum_{i=1}^Nexp(-y_i(f_{m-1}(x_i)+\\alpha G(x_i)))  （20）$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式可以表示为：$$(\\alpha_m,G_m(x))=argmin_{\\alpha,G}\\sum_{i=1}^N\\bar{w}_{mi}exp[-y_i\\alpha G(x_i)] （21）$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$\\bar{w}_{mi}=exp[-y_if_{m-1}(x_i)]$,因为$\\bar{w}_{mi}$既不依赖于$\\alpha$也不依赖于$G$,所以与最小化无关.但是$\\bar{w}_{mi}$依赖于$f_{m-1}(x)$，随着每一轮迭代而发生变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面证明，使得式（21）式达到最小的$\\alpha^*_m$，$G_m^\\ast(x) $就是AdaBoost算法所得到的$\\alpha_m$和$G_m(x)$，求解（21）式分两步**："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，求$G_m^\\ast(x)$，对任意$\\alpha>0$，使式(21)最小的$G(x)$由下式得到：$$G_m^\\ast(x)=argmin_G\\sum_{i=1}^N\\bar{w}_{mi}I(y_i\\neq G(x_i))（22）$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$\\bar{w}_{mi}=exp[-y_if_{m-1}(x_i)]$，这里（22）和（21）式实际上是等价的,因为对$\\sum_{i=1}^N\\bar{w}_{mi}exp[-y_i\\alpha G(x_i)]$求最小值，$\\alpha>0$,$\\bar{w}_{mi}$可看为常量,所以只用每个$y_iG(x_i)$最大，那么也就是尽量让$I(y_i\n",
    "\\neq G(x_i))$的在整个数据集上的数量最小.也就是下面这句话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*此分类器$G_m^\\ast(x)$即为AdaBoost算法的基分类器$G_m(x)$,因为他是使得第$m$轮加权训练数据分类误差率最下的基本分类器.$\\bar{w}_{mi}$就是数据权值*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后求解$\\alpha_m^\\ast$，式(21)可写成如下：$$\\sum_{i=1}^N\\bar{w}_{mi}exp[-y_i\\alpha G(x_i)]$$ $$=\\sum_{G_m(x_i)=y_i}\\bar{w}_{mi}e^{-\\alpha}+\\sum_{G_m(x_i)\\neq y_i}\\bar{w}_{mi}e^{\\alpha}$$\n",
    "$$=(e^\\alpha-e^{-\\alpha})\\sum_{i=1}^N\\bar{w}_{mi}I(y_i\\neq G(x_i))+e^{-\\alpha}\\sum_{i=1}^N\\bar{w}_{mi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上式对$\\alpha$求导：简化为$$((e^\\alpha-e^{-\\alpha})h_1+e^{-\\alpha}h_2)'$$令$e^\\alpha=x$求导并使导数为$0$可以得$x^2=\\frac{h_2-h_1}{h_1}$,然后解出$\\alpha_m^\\ast=\\frac{1}{2}log\\frac{h_2-h_1}{h_1}$,最后$$\\alpha_m^\\ast=\\frac{1}{2}log\\frac{1-e_m}{e_m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$e_m=\\frac{\\sum_{i=1}^N\\bar{w}_{mi}I(y_i\\neq G(x_i))}{\\sum_{i=1}^N\\bar{w}_{mi}}=\\sum_{i=1}^Nw_{mi}I(y_i\\neq G_m(x_i))$表示分类误差率，*这与AdaBoot的$\\alpha_m$完全一样*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*在看样本权值更新是否与AdaBoost一样*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由$f_m(x)=f_{m-1}(x)+\\alpha_mG_m(x)$和$\\bar{w}_{mi}=exp[-y_if_{m-1}(x_i)]$,那么有$$\\bar{w}_{m+1,i}=exp[-y_if_{m}(x_i)]=exp[-y_i(f_{m-1}(x)+\\alpha_mG_m(x))]=exp[-y_if_{m-1}(x)]exp[-y_i\\alpha_mG_m(x)]$$ $$\\bar{w}_{m+1,i}=\\bar{w}_{m,i}exp[-y_i\\alpha_mG_m(x)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这也与AdaBoost算法权值更新只差了规范化因子，所以等价.   \n",
    "至此证毕"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost算法的正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了防止AdaBoost过拟合，通常会加入正则化项，通常是加入一个步长$v$，即$$f_m(x)=f_{m-1}(x)+v\\alpha_mG_m(x)$$ $v$的取值范围$(0,1)$，较小的$v$意味需要更多弱学习器的迭代次数，通常用步长和最大迭代次数一起决定算法拟合效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （二）提升树(Boosting Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提升树模型被认为是统计学习中性能最好的方法之一."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提升树模型   \n",
    "以决策树(回归树)为基分类器的提升方法称为提升树(Boosting Tree).决策树或回归树都是二叉树.提升树模型可以表示为决策树的加法模型：$$f_M(x)=\\sum_{m=1}^MT(x;\\Theta_m)$$其中$T(x;\\Theta_m)$表示决策树;$\\Theta$表示决策树参数；M为决策树个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提升树算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第$m$步模型是:$$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经验风险极小化:$$\\hat{\\Theta}_m=argmin_{\\Theta_m}\\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x;\\Theta_m))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#FF0000>提升回归树使用平方误差作为损失函数，提升分类树使用指数损失函数($L(y,f(x))=exp(-yf(x))$,P145),以及一般损失函数的一般问题</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二分类问题，提升树算法只需要将AdaBoost算法中基本分类器限制为二分类决策树就可以.<font color=#FF0000>虽然损失函数是指数函数，事实上是不用直接计算这个损失函数的</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 提升回归树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知训练数据$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归问题提升树使用以前向分布算法：$$f_0(x)=0$$ $$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m),m=1,2,...,M$$  $$f_M(x)=\\sum_{m=1}^MT(x;\\Theta_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前向分布算法第m步，给定当前模型$f_{m-1}(x)$需求解：$$\\hat{\\Theta}_m=argmin_{\\Theta_m}\\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x;\\Theta_m))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中采用平方误差损失函数时，$$L(y,f(x))=(y-f(y))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即$$L(y,f_{m-1}(x)+T(x;\\Theta_m))$$  $$=[y-f_{m-1}(x)-T(x;\\Theta_m)]^2$$ $$=[(r-T(x;\\Theta))]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里， $$r=y-f_{m-1}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表示<font color=#FF0000>当前模型拟合数据的残差,所以对于回归算法来说，只需要简单拟合当前模型的残差</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提升回归树算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入:训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$  \n",
    "输出:提升树$f_M(x)$  \n",
    "(1)初始化$f_0(x)=0$   \n",
    "(2)对$m=1,2,...,M$：   \n",
    "$\\quad\\quad$a.计算残差$$r_{mi}=y_i-f_{m-1}(x_i),i=1,2,...,N$$\n",
    "$\\quad\\quad$b.拟合残差$r_{mi}$学习一个回归树，得到$T(x;\\Theta_m)$   \n",
    "$\\quad\\quad$c.更新$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$   \n",
    "(3)得到提升回归树$$f_M(x)=\\sum_{m=1}^MT(x;\\Theta_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （三） 梯度提升(gradient boosting decision tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#FF0000>当损失函数是平方损失和指数函数损失函数，每一步优化很简单，但对于一般的损失函数而言往往不易</frpnt>这就引入了梯度提升，利用最速下降法的近似方法，**关键是使用损失函数的负梯度在当前模型的值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度提升树算法（GBDT）-回归算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$;损失函数$L(y,f(x))$;   \n",
    "输出：回归树$\\hat{f}(x)$  \n",
    "(1)初始化$$f_0(x)=argmin_c\\sum_{i=1}^NL(y_i,c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)对$m=1,2,...,M$:  \n",
    "$\\quad\\quad$a.对$i=1,2,...,N$计算$$r_{mi}=-[\\frac{\\partial L(y_i,f(x_i)) }{\\partial f(x_i) }]_{f(x)=f_{m-1}(x)}$$  \n",
    "$\\quad\\quad$b.对新数据集$(x_i,r_{mi})$拟合一个CART回归树，注意里标签值是$r_{mi}$，得到第$m$棵树的叶节点区域$R_{mj},j=1,2,..,J$   \n",
    "$\\quad\\quad$c.对$j=1,2,...,J$,计算$$c_{mj}=argmin_c \\sum_{x_i\\in R_{mj}}L(y_i,f_{m-1}(x_i)+c)（CART回归树中是各个区域标签均值）$$  \n",
    "$\\quad\\quad$得到回归树$$T_{m}=\\sum_{m=1}^Jc_{mj}I(x\\in R_{mj})$$\n",
    "$\\quad\\quad$d.更新$f_m(x)=f_{m-1}(x)+\\sum_{j=1}^Jc_{mj}I(x\\in R_{mj})$   \n",
    "(3)得到回归树$$\\hat{f}(x)=f_M(x)=\\sum_{m=1}^M\\sum_{j=1}^Jc_{mj}I(x\\in R_{mj})$$   \n",
    "注意:   \n",
    "1.算法第一步初始化，估计使损失函数极小化的常数值，它是只有根节点的树      \n",
    "2.第2(a)步中计算损失函数的负梯度在当前模型的值，将它作为残差估计，对于平方损失函数，它就是通常所说的残差，对于一般损失函数，它就是残差近似值   \n",
    "3.第2(b)步估计回归树叶节点区域，以拟合残差近似值   \n",
    "4.第2(c)步利用线性搜索估计叶节点区域的值，使损失函数极小化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>7.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>8.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>8.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>9.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x     y\n",
       "0   1  5.56\n",
       "1   2  5.70\n",
       "2   3  5.91\n",
       "3   4  6.40\n",
       "4   5  6.80\n",
       "5   6  7.05\n",
       "6   7  8.90\n",
       "7   8  8.70\n",
       "8   9  9.00\n",
       "9  10  9.05"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data0={\"x\":[1,2,3,4,5,6,7,8,9,10],\n",
    "     \"y\":[5.56,5.70,5.91,6.40,6.80,7.05,8.90,8.70,9.00,9.05]}#来自统计学习方法\n",
    "data0=pd.DataFrame(data0)\n",
    "data0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来计算其GBRT模型,选取的最小化整体损失函数是$$\\sum_{i=1}^N(y_i-f(x_i))^2$$,其单个样本损失函数为$$(y_i-f(x_i))^2$$梯度为$$-[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}]=2(y_i-f(x_i))$$这里直接使用$y_i-f(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步先求$f_0(x)$即回归树$T_0(x)$，也就是求$f(x)=argmin_{c_0} \\sum_{i=0}^9(y_i-c_0)^2$,对此式求导可得$c_0=\\frac{\\sum_{i=0}^Ny_i}{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.307\n",
      "[19.114210000000003]\n"
     ]
    }
   ],
   "source": [
    "data_y=data0[\"y\"]\n",
    "data_L0=[]\n",
    "c_0=np.sum(data0[\"y\"])/len(data0)\n",
    "print(c_0)\n",
    "data_L0.append(np.sum((data_y-c_0)**2))\n",
    "print(data_L0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上面计算当我们选取$f_0(x)=7.05$，损失函数有最小值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步根据第2a步求出残差表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1.593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1.743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x      r\n",
       "0   1 -1.747\n",
       "1   2 -1.607\n",
       "2   3 -1.397\n",
       "3   4 -0.907\n",
       "4   5 -0.507\n",
       "5   6 -0.257\n",
       "6   7  1.593\n",
       "7   8  1.393\n",
       "8   9  1.693\n",
       "9  10  1.743"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_0=7.307\n",
    "data1={\"x\":[1,2,3,4,5,6,7,8,9,10],\n",
    "     \"r\":list(data0[\"y\"]-f_0)}\n",
    "data1=pd.DataFrame(data1)\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三步对残差进行拟合回归树(见CART回归树的建立):   \n",
    "首先计算搜索最优切分特征和最优切分点，由于特征就一个所以就是$x$，现在搜索最优切分点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.723088888888887, 12.0833875, 8.365638095238095, 5.775475000000003, 3.9113200000000017, 1.9300083333333335, 8.009809523809526, 11.735399999999997, 15.738599999999998, 19.11421]\n",
      "c1: -1.0703333333333338\n",
      "c2: 1.6054999999999997\n"
     ]
    }
   ],
   "source": [
    "data_L1=[]\n",
    "C11=[]\n",
    "C12=[]\n",
    "# S=[(data1[\"x\"][i]+data1[\"x\"][i+1])/2 for i in  range(len(data1[\"x\"])-1)]#备选切分点,这里用的是x间隔均值\n",
    "# print(S)\n",
    "# for s in S:\n",
    "#     sum=0\n",
    "#     c1=[]\n",
    "#     c2=[]\n",
    "#     for x,r in zip(data1[\"x\"],data1[\"r\"]):\n",
    "#         if x<=s:\n",
    "#             c1.append(r)\n",
    "#         else:\n",
    "#             c2.append(r)\n",
    "#     c1=np.mean(c1)\n",
    "#     c2=np.mean(c2)\n",
    "#     for x,r in zip(data1[\"x\"],data1[\"r\"]):\n",
    "#         if x<=s:\n",
    "#             sum+=(r-c1)**2\n",
    "#         else:\n",
    "#             sum+=(r-c2)**2\n",
    "#     data_L1.append(sum)\n",
    "\n",
    "##直接选取x作为备选切分点\n",
    "for s in data1[\"x\"]:\n",
    "    sum=0\n",
    "    c11=[]\n",
    "    c12=[]\n",
    "    for x,r in zip(data1[\"x\"],data1[\"r\"]):\n",
    "        if x<=s:\n",
    "            c11.append(r)\n",
    "        else:\n",
    "            c12.append(r)\n",
    "    c11=np.mean(c1)\n",
    "    c12=np.mean(c2)\n",
    "    C11.append(c1)\n",
    "    C12.append(c2)\n",
    "    for x,r in zip(data1[\"x\"],data1[\"r\"]):\n",
    "        if x<=s:\n",
    "            sum+=(r-c11)**2\n",
    "        else:\n",
    "            sum+=(r-c12)**2\n",
    "    data_L1.append(sum)\n",
    "print(data_L1)\n",
    "print(\"c11:\",C11[5])\n",
    "print(\"c12:\",C12[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以得知最优切分点是$x=6$,个各区域输出值为$c_{11}=-1.0703,c_{12}=1.6055$,所以我们可以得到回归树$f_1(x_i)=7.307+\\sum_{k=1}^2c_{1k}I(x_i\\in R_{1k})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一轮迭代已经完成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行第二轮迭代，首先还是计算新的需要拟合的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(x):\n",
    "    if x<=6:\n",
    "        return 7.307-1.0703\n",
    "    else:\n",
    "        return 7.307+1.6055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求新残差表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.6767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.5367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.1633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.8133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.0125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.1375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x       r\n",
       "0   1 -0.6767\n",
       "1   2 -0.5367\n",
       "2   3 -0.3267\n",
       "3   4  0.1633\n",
       "4   5  0.5633\n",
       "5   6  0.8133\n",
       "6   7 -0.0125\n",
       "7   8 -0.2125\n",
       "8   9  0.0875\n",
       "9  10  0.1375"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2={\"x\":[1,2,3,4,5,6,7,8,9,10],\n",
    "     \"r\":[data0[\"y\"][i]-F1(data0[\"x\"][i]) for i in range(10)]}\n",
    "data2=pd.DataFrame(data2)\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三步对残差进行拟合回归树(见CART回归树的建立):\n",
    "首先计算搜索最优切分特征和最优切分点，由于特征就一个所以就是𝑥，现在搜索最优切分点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.421235199999999, 1.0098567799999985, 0.8006163352380933, 1.1402758533333317, 1.665360511999999, 1.930008333333334, 1.9299332152380952, 1.89835646, 1.9089952799999999, 1.9300083360000004]\n",
      "c21: -0.5133666666666675\n",
      "c22: 0.2199857142857142\n"
     ]
    }
   ],
   "source": [
    "data_L2=[]\n",
    "C21=[]\n",
    "C22=[]\n",
    "for s in data2[\"x\"]:\n",
    "    sum=0\n",
    "    c21=[]\n",
    "    c22=[]\n",
    "    for x,r in zip(data2[\"x\"],data2[\"r\"]):\n",
    "        if x<=s:\n",
    "            c21.append(r)\n",
    "        else:\n",
    "            c22.append(r)\n",
    "    c21=np.mean(c21)\n",
    "    c22=np.mean(c22)\n",
    "    C21.append(c21)\n",
    "    C22.append(c22)\n",
    "    for x,r in zip(data2[\"x\"],data2[\"r\"]):\n",
    "        if x<=s:\n",
    "            sum+=(r-c21)**2\n",
    "        else:\n",
    "            sum+=(r-c22)**2\n",
    "    data_L2.append(sum)\n",
    "print(data_L2)\n",
    "print(\"c21:\",C21[2])\n",
    "print(\"c22:\",C22[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以得知最优切分点是$x=3$,个各区域输出值为$c_{21}=-0.5134,c_{22}=0.22$,所以我们可以得到回归树$f_2(x_i)=7.307+\\sum_{k=1}^2c_{1k}I(x_i\\in R_1k)+\\sum_{k=1}^2c_{2k}I(x_i\\in R_{2k})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行第三轮迭代，首先还是计算新的需要拟合的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F2(x):\n",
    "    f1=F1(x)\n",
    "    if x<=3:\n",
    "        return f1-0.5134\n",
    "    else:\n",
    "        return f1+0.22\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求新残差表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.1633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.0233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.0567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.3433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.5933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.2325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.4325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.1325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.0825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x       r\n",
       "0   1 -0.1633\n",
       "1   2 -0.0233\n",
       "2   3  0.1867\n",
       "3   4 -0.0567\n",
       "4   5  0.3433\n",
       "5   6  0.5933\n",
       "6   7 -0.2325\n",
       "7   8 -0.4325\n",
       "8   9 -0.1325\n",
       "9  10 -0.0825"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3={\"x\":[1,2,3,4,5,6,7,8,9,10],\n",
    "     \"r\":[data0[\"y\"][i]-F2(data0[\"x\"][i]) for i in range(10)]}\n",
    "data3=pd.DataFrame(data3)\n",
    "data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三步对残差进行拟合回归树(见CART回归树的建立):\n",
    "首先计算搜索最优切分特征和最优切分点，由于特征就一个所以就是𝑥，现在搜索最优切分点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7709864622222217, 0.7788541149999993, 0.8006163352380948, 0.7992815233333329, 0.767737584, 0.47794967333333394, 0.6009705066666668, 0.771725715, 0.7930538399999999, 0.8006163399999996]\n",
      "c31: 0.14666666666666592\n",
      "c32: -0.2200000000000002\n"
     ]
    }
   ],
   "source": [
    "data_L3=[]\n",
    "C31=[]\n",
    "C32=[]\n",
    "for s in data3[\"x\"]:\n",
    "    sum=0\n",
    "    c31=[]\n",
    "    c32=[]\n",
    "    for x,r in zip(data3[\"x\"],data3[\"r\"]):\n",
    "        if x<=s:\n",
    "            c31.append(r)\n",
    "        else:\n",
    "            c32.append(r)\n",
    "    c31=np.mean(c31)\n",
    "    c32=np.mean(c32)\n",
    "    C31.append(c31)\n",
    "    C32.append(c32)\n",
    "    for x,r in zip(data3[\"x\"],data3[\"r\"]):\n",
    "        if x<=s:\n",
    "            sum+=(r-c31)**2\n",
    "        else:\n",
    "            sum+=(r-c32)**2\n",
    "    data_L3.append(sum)\n",
    "print(data_L3)\n",
    "print(\"c31:\",C31[5])\n",
    "print(\"c32:\",C32[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以得知最优切分点是$x=6$,个各区域输出值为$c_{31}=0.1467,c_{32}=-0.22$,所以我们可以得到回归树$f_3(x_i)=7.307+\\sum_{k=1}^2c_{1k}I(x_i\\in R_1k)+\\sum_{k=1}^2c_{2k}I(x_i\\in R_{2k})+\\sum_{k=1}^2c_{3k}I(x_i\\in R_{3k})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样继续迭代下去直至满足停止条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度提升树算法-分类\n",
    "GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本标签不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。\n",
    "#### 梯度提升树算法（GBDT）-二分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二元分类GBDT,这里使用类似于逻辑回归的对数似然损失函数，则单样本损失函数为：$$L(y,f(x))=-logP(Y=y|x)=log(1+exp(-yf(x)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*这个损失函数这么来的：*   \n",
    "对于$x_i$,其标签是$y_i$,那么我们当然希望模型判断$x_i$为$y_i$的概率$P(Y=y_i|x_i)$越大越好，也就是希望$-P(Y=y_i|x_i)$越小越好。在二分类中$y\\in \\{-1,+1\\}$，$$P(Y=y|x)=\\frac{exp(yf(x))}{1+exp(yf(x))}$$所以有：\n",
    "$$P(Y=1|x)=\\frac{exp(f(x))}{1+exp(f(x))}$$，$$P(Y=-1|x)=1-P(Y=1|x)=\\frac{exp(-f(x))}{1+exp(-f(x))}=\\frac{1}{1+exp(f(x))}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以可以得到$$L(y,f(x))=-logP(Y=y|x)=-log(\\frac{exp(yf(x))}{1+exp(yf(x))})=log(\\frac{1+exp(yf(x))}{exp(yf(x))})=log(1+exp(-yf(x)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则此时的负梯度误差为$$r_{mi}=-[\\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)}]_{f(x)=f_{m-1}(x)}=\\frac{y_i}{1+exp(y_if(x_i))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于生成的决策树，各个叶子节点的最佳负梯度拟合值为$$c_{mk}=argmin_c\\sum_{x_i\\in R_{mk}}log(1+exp(-y_i(f_{t-1}(x_i)+c)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式难解，一般使用近似值代替$$c_{mk}=\\frac{\\sum_{x_i\\in R_{mk}}r_{mi}}{\\sum_{x_i\\in R_{mi}}}|r_{mk}|(1-|r_{mk}|)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是除了负梯度计算和叶子结点的最佳负梯度拟合，二元GBDT分类同GBDT回归算法过程相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面推导中若$y\\in \\{1,0\\}$，则单样本损失函数可以写成逻辑回归的对数似然函数$$L(y,f(x))=ylog(P(Y=1|x))+(1-y)log(1-P(Y=1|x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$$P(Y=1|x)=\\frac{exp(f(x))}{1+exp(f(x))}$$，$$P(Y=0|x)=1-P(Y=1|x)=\\frac{1}{1+exp(f(x))}$$.就不能写为$P(Y=y|x)=\\frac{exp(yf(x))}{1+exp(yf(x))}$的合体形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度提升树算法（GBDT）-多分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：   \n",
    "1.对于多分类任务，GDBT的做法是采用一对多的策略,即，对每个类别训练$M$个分类器。假设有$K$个类别，那么训练完之后总共有$M\\times K$颗树。   \n",
    "2.$K$个类别都拟合完第一颗树之后才开始拟合第二颗树，不允许先把某一个类别的M颗树学习完，再学习另外一个类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于多元分类，我们假设有$K$类，则此时的单样本对数似然损失函数函数$$L(y,f(x))=-\\sum_{k=1}^Ky_klog(P(Y=k|x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整体的就是$$L(y,f(x))=-\\sum_{i=1}^Ny_ilog(P(Y=y_i|x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中如若模型对样本$x$输出类别为$y_k$则$y_k=1$,否则$y_k=0$，也就是实际上只算了其标签类别对应的预测值得损失函数$$P(Y=k|x)=\\frac{exp(f_k(x))}{\\sum_{l=1}^Kexp(f_l(x))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是实际上模型输出的是一个K维向量，表示样例属于各个类别的相对可能性，然后通过上式进行归一化处理."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，在第$m$轮迭代，第$i$个样本对应类别$l$的负梯度误差是$$r_{mil}=-[\\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)}]_{f(x)=f_{m-1}l(x)}=y_{il}-P(Y=l|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于各个类别生成的决策树，各个叶子节点的最佳负梯度拟合值为$$c_{mlk}=argmin_{c}\\sum_{x_i\\in R_mlk}L(l,f_{m-1,l}(x)+c)$$$c_{mlk}$就表示$l$类别的第$m$轮迭代的决策树的第$k$个分区的拟合值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于上式难优化，所以用近似值代替$$c_{mlk}=\\frac{K-1}{K}\\frac{\\sum_{x_i\\in R_{mlk}}r_{mlk}}       {\\sum_{x_i\\in R_{mlk}}|r_{mlk}|(1-|r_{mlk}|)}$$这里K表示类别数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么就可以得到$l$类别第$m$轮迭代的决策树$$T_{ml}=\\sum_{i=1}^Nc_{mlk}I(x_i\\in R_{mlk})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么当所有类别的第$m$轮迭代分类器都生成了之后再继续下一轮迭代"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
